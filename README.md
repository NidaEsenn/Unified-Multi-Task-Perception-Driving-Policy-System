# Unified-Multi-Task-Perception-Driving-Policy-System
A lightweight student-built autonomous driving foundation stack featuring multi-task perception (YOLOv8 + U-Net), a temporal end-to-end driving policy network (ConvLSTM), and a data-engine pipeline for frame curation and real-time evaluation.
# ğŸš— Autonomous Driving Foundation Stack (Student Edition)
<img width="1536" height="1024" alt="Autonomous Driving Foundation Stack Design" src="https://github.com/user-attachments/assets/76af247e-8548-42a9-85e8-8a549ec37210" />

A lightweight autonomous driving foundation stack designed for research and educational purposes.  
This project demonstrates a unified approach to **multi-task perception**, **temporal end-to-end control**, and **data-engine curation**â€”all core components used in modern self-driving systems.

Built using **Python, PyTorch, OpenCV, YOLOv8, U-Net, and ConvLSTM**, this stack provides a clean foundation for students exploring perception, planning, and imitation-learningâ€“based driving policies.

---

## ğŸ“Œ Key Features

### ğŸ§© Multi-Task Perception Module
- **Vehicle Detection** using YOLOv8  
- **Lane Segmentation** using a U-Net architecture  
- **Drivable Area Prediction** for scene understanding  
- Shared encoder design for efficient multi-task learning  
- Generates frame-by-frame perception overlays  

### ğŸ® End-to-End Driving Policy (Imitation Learning)
- Temporal model built with **ConvLSTM**  
- Predicts **steering commands** from sequential image frames  
- Trained on curated driving datasets  
- Incorporates motion/temporal patterns absent in single-frame models  

### ğŸ§  Data Engine â€” Frame Curation Pipeline
- Filters **blurry**, **overexposed**, and **low-information** frames  
- Edge-case mining using:
  - Steering change magnitude  
  - Frame-to-frame motion  
  - Brightness/contrast scores  
- Outputs a curated subset for high-quality model training  

### ğŸ¥ Real-Time Demo System
- Overlays:
  - detected vehicles  
  - lane boundaries  
  - drivable area mask  
  - predicted steering angle  
- Rendered as a single annotated driving video for evaluation  

---

## ğŸ—ï¸ Architecture Overview
                    +--------------------------+
                    |      Input Frame(s)      |
                    +-------------+------------+
                                  |
            +---------------------+--------------------+
            |                                          |
    +-------v--------+                        +--------v---------+
    |  Perception     |                        |  Driving Policy  |
    | (YOLO + U-Net)  |                        |   (ConvLSTM)     |
    +-------+--------+                        +--------+---------+
            |                                          |
            +---------------------+--------------------+
                                  |
                    +-------------v------------+
                    |   Real-Time Visualizer   |
                    +--------------------------+

---

## ğŸ“‚ Repository Structure

Autonomous-Driving-Foundation-Stack/
â”‚
â”œâ”€â”€ perception/
â”‚ â”œâ”€â”€ vehicle_detection_yolov8.py
â”‚ â”œâ”€â”€ lane_unet.py
â”‚ â”œâ”€â”€ drivable_area_unet.py
â”‚
â”œâ”€â”€ policy/
â”‚ â”œâ”€â”€ convlstm_model.py
â”‚ â”œâ”€â”€ train_policy.py
â”‚ â”œâ”€â”€ evaluate_policy.py
â”‚
â”œâ”€â”€ data_engine/
â”‚ â”œâ”€â”€ curate_frames.py
â”‚ â”œâ”€â”€ quality_metrics.py # blur, motion, brightness
â”‚
â”œâ”€â”€ realtime_demo/
â”‚ â”œâ”€â”€ overlay_demo.py
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ raw_videos/
â”‚
â”œâ”€â”€ results/
â”‚ â”œâ”€â”€ demo_output.mp4
â”‚ â”œâ”€â”€ sample_overlays/
â”‚
â””â”€â”€ README.md


---

## ğŸš€ Getting Started

### 1. Install Dependencies
```bash
pip install -r requirements.txt

### 2. Run Perception Module
python perception/vehicle_detection_yolov8.py
python perception/lane_unet.py

### 3. Curate Dataset With Data Engine
python data_engine/curate_frames.py --input data/raw_videos/video.mp4
### 4. Train End-to-End Driving Policy
python policy/train_policy.py --data curated/
### 5. Generate Real-Time Demo Output
python realtime_demo/overlay_demo.py



ğŸ¯ Future Improvements
Add BEV (Birdâ€™s-Eye View) transformation
Introduce reinforcement learning for fine-tuned control
Fuse multiple sensors (additional cameras or pseudo-LiDAR)
Integrate lightweight transformer-based temporal models
ğŸ™Œ Acknowledgements
This project was built as a student-oriented foundation stack to explore core concepts behind modern autonomous driving systems and multi-task perception networks.




